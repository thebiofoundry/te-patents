{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Input Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that appears in more than x% of all documents\n",
    "extremes_no_above=0.70 # i.e. Filter out terms appearing in more than 90% of documents. Higher means less words removed.\n",
    "\n",
    "# Filter out words that appears in less than n documents (this is a number, not %!)\n",
    "extremes_no_below=10 # i.e. Filter out terms appearing in less than 10 documents. Lower means less words removed\n",
    "\n",
    "# TF IDF low value words removal threshold. \n",
    "tfidf_low_value = 0.020 # Set higher to remove more words. You can see what words are removed below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "chunksize = 10000\n",
    "passes = 40 # default is one pass\n",
    "iterations = 500\n",
    "eval_every = None  # For logging, to save time, put None to not evaluate model perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set number seed for reproducibility\n",
    "seed_number = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For monitoring convergence\n",
    "# https://stackoverflow.com/questions/37570696/how-to-monitor-convergence-of-gensim-lda-model\n",
    "import logging\n",
    "logging.basicConfig(filename='gensim.log', format=\"%(asctime)s:%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob\n",
    "import nltk\n",
    "import gensim\n",
    "import string\n",
    "from unicodedata import category\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"full_lens_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Abstract'].dropna().astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with tokenization\n",
    "X_tokenized = X.apply(nltk.word_tokenize)\n",
    "\n",
    "# Take a look at top tokens by frequency\n",
    "pd.Series(np.concatenate(X_tokenized.values)).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FInd out number of tokens\n",
    "len(pd.Series(np.concatenate(X_tokenized.values)).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the default common english stopwords (used by the library)\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "# sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert all tokens to lower case\n",
    "X_tokenized = X_tokenized.apply(lambda row: [word.lower() for word in row])\n",
    "\n",
    "# Remove single character tokens\n",
    "X_tokenized = X_tokenized.apply(lambda row: [word for word in row if len(word) > 1])\n",
    "\n",
    "# Remove punctuation\n",
    "X_tokenized = X_tokenized.apply(lambda row: [word for word in row if word not in sw and word not in string.punctuation])\n",
    "\n",
    "# Remove other unneeded stuff\n",
    "custom_sw = [u'\\'\\'', u'``', 'r']\n",
    "X_tokenized = X_tokenized.apply(lambda row: [word for word in row if word not in custom_sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at top n words\n",
    "pd.Series(np.concatenate(X_tokenized.values)).value_counts().head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize dataset\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "X_tokenized = X_tokenized.apply(lambda x: [wordnet_lemmatizer.lemmatize(y) for y in x])\n",
    "\n",
    "# Take a look at top 50 words\n",
    "pd.Series(np.concatenate(X_tokenized.values)).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Find bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams and trigrams\n",
    "\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear extremes_no_below times or more).\n",
    "bigram = Phrases(X_tokenized, min_count=extremes_no_below, delimiter=b'_', threshold=6)\n",
    "trigram = Phrases(bigram[X_tokenized.tolist()], min_count=extremes_no_below, delimiter=b'_', threshold=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see what tokens are generated in the second document, index #1\n",
    "# X_tokenized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see what tokens are generated in the second document, index #1\n",
    "# bigram[X_tokenized[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see what tokens are generated in the second document, index #1\n",
    "# trigram[bigram[X_tokenized[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tokenized_bitrigrams = []\n",
    "\n",
    "for idx in range(len(X_tokenized)):\n",
    "    X_tokenized_bitrigrams.append([])\n",
    "    for token in bigram[X_tokenized[idx]]:\n",
    "        if token.count('_') == 1:\n",
    "            # Token is a bigram, add to document.\n",
    "            X_tokenized_bitrigrams[idx].append(token)\n",
    "    for token in trigram[bigram[X_tokenized[idx]]]:\n",
    "        if token.count('_') == 2:\n",
    "            # Token is a trigram, add to document.\n",
    "            X_tokenized_bitrigrams[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all the bigrams and trigrams (row = document #, columns = bigrams trigrams in that particular document)\n",
    "pd.DataFrame(X_tokenized_bitrigrams).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Append the bigrams and trigrams to the training set\n",
    "for idx in range(len(X_tokenized_bitrigrams)):\n",
    "    for token in X_tokenized_bitrigrams[idx]:\n",
    "        X_tokenized[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at top n words\n",
    "pd.Series(np.concatenate(X_tokenized.values)).value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tokens do we have?\n",
    "len(pd.Series(np.concatenate(X_tokenized.values)).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Manual and extreme word filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dictionary before other manual and extremes filtering\n",
    "dictionary = corpora.Dictionary(X_tokenized)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Manual filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set('')\n",
    "# List of the wordsto be removed manually\n",
    "stoplist = set(\"tissue invention engineering method methods preparation used present relates solution one said herein embodiment\\\n",
    "                adding comprises tissue_engineering  application present_invention also preparation_method provided first second \\\n",
    "                bone cornea corneal skin cartilage stent lung vascular heart cancer immune tissue-engineered muscle organ described_herein \\\n",
    "                connective dermal hair liver head spine cord producing end include derivative tissue_construct present_invention_relates \\\n",
    "                seed_cell bone_tissue_engineering present_invention_provides e.g. cardiac within prepare easy example sample patient utility_model \\\n",
    "                mean based surgical selected\".split())\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "dictionary.filter_tokens(stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the remainingwords after manual filtering \n",
    "for k, id in dictionary.items():\n",
    "    print(k, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate corpus\n",
    "corpus = [dictionary.doc2bow(document) for document in X_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Filtering using TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words with low TF IDF values on a per document level\n",
    "tfidf = models.TfidfModel(corpus, id2word = dictionary)\n",
    "\n",
    "#Filter low value words and also words missing in tfidf models.\n",
    "low_value = tfidf_low_value\n",
    "filtered_words = []\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    filtered_words.append(low_value_words)\n",
    "\n",
    "    #reassign\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_value_words = []\n",
    "for doc in filtered_words:\n",
    "    low_value_words.append([dictionary.id2token[word] for word in doc if len(doc) > 0])\n",
    "low_value_df = pd.Series(low_value_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show words removed by tf_idf \n",
    "# low_value_df.aloc[low_value_df.str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique number of tokens to be removed\n",
    "low_value_words_list = [item for sublist in low_value_words for item in sublist]\n",
    "len(set(low_value_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(low_value_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low value words in our dictionary\n",
    "low_value_ids = [dictionary.token2id[word] for word in low_value_words_list]\n",
    "dictionary.filter_tokens(bad_ids=set(low_value_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words remaining\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Extremes filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words appearing in less than n documents or in more than x% of the documents\n",
    "dictionary.filter_extremes(no_below=extremes_no_below, no_above=extremes_no_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words remaining after all filtering\n",
    "for k, id in dictionary.items():\n",
    "    print(k, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute corpus with the extremes and low valued words filtered out\n",
    "corpus = [dictionary.doc2bow(doc) for doc in X_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Coherence scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    print('Please wait...')\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print('Evaluating # topics: ', num_topics)\n",
    "        model = gensim.models.LdaMulticore(corpus=corpus, random_state=seed_number , id2word=dictionary, chunksize=chunksize, \\\n",
    "                               iterations=iterations, num_topics=num_topics, \\\n",
    "                               passes=passes, eval_every=eval_every)        \n",
    "        model_list.append(model)\n",
    "        coherencemodel = gensim.models.coherencemodel.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models?\n",
    "limit=5; start=2; step=1;\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=X_tokenized, start=start, limit=limit, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out_x\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "x = range(start, limit, step)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, coherence_values)\n",
    "ax.set_xlabel(\"Num Topics\")\n",
    "ax.set_ylabel(\"Coherence score\")\n",
    "ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "plt.grid(b=True, which='both', color='0.85', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "out_x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LDA model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics to be chosen based on the coherence value\n",
    "ldamodel = model_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture topics_bubble\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=True, R=30)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_bubble.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Post Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of topics acroos all documents\n",
    "dict(ldamodel.get_document_topics(corpus[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = []\n",
    "\n",
    "for c in corpus:\n",
    "    probabilities.append(dict(ldamodel.get_document_topics(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop = pd.DataFrame(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframe and rename the columns\n",
    "df_prop = pd.DataFrame(probabilities)\n",
    "df_prop.columns = [str('Topic {}'.format(i+1)) for i in range(len(df_prop.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[['Title', 'Applicants', 'Inventors', 'URL', 'Abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_subset.join(df_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.to_csv('Lens_Data_With_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink('Lens_Data_With_Topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Organ distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the organ-specific keywords and count number of patents focusing on each organ\n",
    "organs = ['bone' ,\n",
    "          'cartilage' ,\n",
    "          'dental' ,\n",
    "          'heart' ,\n",
    "          'vascular' ,\n",
    "          'cardiovascular' , \n",
    "          'adipose' ,\n",
    "          'tendon' ,\n",
    "          'ligament' ,\n",
    "          'connective' ,\n",
    "          'skin' ,\n",
    "          'hair' ,\n",
    "          'immune' ,\n",
    "          'soft' ,\n",
    "          'kidney' ,\n",
    "          'bladder' ,\n",
    "          'spine' ,\n",
    "          'spinal_cord' ,\n",
    "          'nerve' ,\n",
    "          'lung' ,\n",
    "          'liver' ,\n",
    "          'pancreas' ,\n",
    "          'reproductive' ,\n",
    "          'muscles' ,\n",
    "          'organoid' , \n",
    "          'ear' ,\n",
    "          'eye' ,\n",
    "          'cancer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_organs = df[['Title', 'Applicants', 'Inventors', 'URL', 'Abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for organ in organs:\n",
    "    df_organs[organ] = df_organs['Abstract'].str.lower().str.contains(organ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_organs.to_csv('Lens_Data_With_Organs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink('Lens_Data_With_Organs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
